{"cells":[{"cell_type":"markdown","source":["# SC3000 Lab Assignment 1: Teaching NanoGPT to Do Math\n","\n","Group members: Siaw Xuan, Ramakrishna Rohan, Law Wei Zhen\n","-\n","\n","---\n","\n","## Summary\n","\n","This notebook applies **Direct Preference Optimization (DPO)** to fine-tune a pretrained NanoGPT for mathematical reasoning. DPO trains the language model to favor better responses over weaker ones—without building a separate reward model.\n","\n","### What is DPO?\n","\n","**Direct Preference Optimization** is a training approach that replaces the RLHF pipeline. Rather than learning a reward model and using reinforcement learning, DPO updates the model directly from paired examples where one response is preferred over another.\n","\n","### Why DPO?\n","\n","* **Simpler pipeline**: Removes the need for a reward model and RL steps\n","* **More stable**: Avoids the instabilities common in RL-based training\n","* **Strong results**: Often matches or beats RLHF performance\n","* **Resource-friendly**: Lower computational overhead\n","\n","### Data Setup\n","\n","Training uses **preference pairs**:\n","\n","* **Positive (preferred)**: Correct solutions with clear reasoning\n","* **Negative (dispreferred)**: Incorrect answers or weak/illogical reasoning\n","\n","The objective pushes the model to assign higher likelihood to positive responses and lower likelihood to negative ones.\n"],"metadata":{"id":"BkzMRRa5EHdz"},"id":"BkzMRRa5EHdz"},{"cell_type":"markdown","source":["## The DPO Algorithm: Mathematical Intuition\n","\n","### Core Idea\n","\n","In classic RLHF, you typically:\n","\n","1. Train a reward model on human preferences\n","2. Use that reward model to score candidate outputs\n","3. Run RL (e.g., PPO) to improve the policy\n","\n","**DPO** collapses this into a *single* stage: it updates the policy (language model) *directly* from preference pairs.\n","\n","### The DPO Loss Function (Explained)\n","\n","Given a preference pair (y_pos, y_neg) where y_pos is preferred over y_neg:\n","\n","$$\\mathcal{L}_{DPO} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_{pos}|x)}{\\pi_{ref}(y_{pos}|x)} - \\beta \\log \\frac{\\pi_\\theta(y_{neg}|x)}{\\pi_{ref}(y_{neg}|x)}\\right)\\right]$$\n","\n","**Term meanings:**\n","\n","* *(\\pi_\\theta)*: the policy we’re training\n","* *(\\pi_{\\text{ref}})*: a reference policy (often the current/initial model)\n","* *(\\beta)*: temperature scaling how strongly preferences shape updates\n","* *(\\sigma)*: the sigmoid mapping the log-odds difference to a probability\n","\n","**Intuition:**\n","\n","* Pushes the model to assign *higher* likelihood to preferred responses\n","* Penalizes giving *higher* likelihood to dispreferred ones\n","* *(\\beta)* adjusts the *strength* of these updates\n","\n","### Why DPO Works\n","\n","1. *Direct optimization*: skips reward-model + RL loops\n","2. *Stable*: avoids RL-induced instability\n","3. *Principled*: connects back to the RLHF objective\n","4. *Efficient*: one-stage training\n","\n","---\n"],"metadata":{"id":"fYV0A6jgGH3V"},"id":"fYV0A6jgGH3V"},{"cell_type":"markdown","id":"124a869a","metadata":{"id":"124a869a"},"source":["## Step 1: Install necessary packages"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wNdLQGy7a5XK"},"id":"wNdLQGy7a5XK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/Colab Notebooks/NanoGPT-Math\""],"metadata":{"id":"6QbDpreGwTMa"},"id":"6QbDpreGwTMa","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3b82f8f1","metadata":{"id":"3b82f8f1"},"outputs":[],"source":["!pip install matplotlib\n","!pip install torch numpy transformers datasets tiktoken wandb tqdm"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"id":"rC-Wa8Euw4hj"},"id":"rC-Wa8Euw4hj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter Choices and Their Impact\n","\n","| Parameter             |           Value | Impact                                                                  |\n","| --------------------- | --------------: | ----------------------------------------------------------------------- |\n","| **beta**              |             0.5 | Preference strength; lower = stronger push, higher = more conservative. |\n","| **learning_rate**     |            1e-4 | Stable fine-tuning; too high risks divergence.                          |\n","| **batch_size**        |              64 | Smoother grads with size; needs more VRAM.                              |\n","| **max_length** |              64 | Caps input; trim long prompts upstream.                                 |\n","| **temperature**       |             0.8 | Randomness: <1 deterministic, >1 diverse.                               |\n","\n","\n","**Key trade-offs:**\n","\n","* **Higher β** → More conservative, closer to reference policy\n","* **Lower β** → More aggressive preference learning (risk of overfitting/drift)\n","* **Larger batch_size** → Smoother gradients, higher VRAM demand\n","* **Higher learning_rate** → Faster convergence, greater instability risk\n","* **Lower temperature / smaller top_k** → Safer, more deterministic generations (possible blandness)\n","* **More epochs / larger max_new_tokens** → Better coverage and completeness, higher compute and overfit risk\n"],"metadata":{"id":"fIY4svJeH-r4"},"id":"fIY4svJeH-r4"},{"cell_type":"markdown","id":"6c2d9de0","metadata":{"id":"6c2d9de0"},"source":["## Step 2: Package imports and configuration\n","\n","### Configuration Parameters\n","\n","* **`beta`**: DPO scaling temperature—tunes how strongly the policy is pushed away from the reference model\n","* **`base_lr`**: Optimizer step size (learning rate)\n","* **`epochs`**: Upper bound on full passes over the training data\n","* **`batch_size`**: Count of preference pairs processed per optimization step\n","* **`max_length`**: Cap on input sequence length\n","* **`temperature`**: Generation randomness control (smaller values → more deterministic outputs)\n","* **`top_k`**: Limits sampling to the top *k* most probable tokens\n"]},{"cell_type":"code","execution_count":null,"id":"876dd92d","metadata":{"id":"876dd92d"},"outputs":[],"source":["import sys\n","import os\n","import torch.nn.functional as F\n","sys.path.append(os.path.abspath(\"..\"))\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","import pickle\n","from model import GPT, GPTConfig\n","import random\n","from tqdm import tqdm\n","import time\n","import json\n","import matplotlib.pyplot as plt\n","\n","# DPO hyperparameters\n","beta = 0.5\n","base_lr = 1e-4\n","epochs = 20\n","batch_size = 64\n","\n","# Model parameters\n","max_length =64\n","num_samples = 1\n","max_new_tokens = 200\n","temperature = 0.8\n","top_k = 200\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","# tokenizer functions\n","with open(\"sft/meta.pkl\", \"rb\") as f:\n","    meta = pickle.load(f)\n","stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n","\n","PAD_ID = len(stoi)          # add new padding token id\n","stoi['<PAD>'] = PAD_ID\n","itos[PAD_ID] = '<PAD>'\n","\n","def encode(s): return [stoi[c] for c in s]\n","def decode(l): return ''.join([itos[i] for i in l])"]},{"cell_type":"markdown","id":"4c7d35e6","metadata":{"id":"4c7d35e6"},"source":["## Step 3: Define Helper Functions\n","\n","We’ll implement three core utilities to support DPO training:\n","\n","### 1. `compute_logprob(input_ids)`\n","\n","Calculates the log-likelihood of a given token sequence under the current model—this feeds directly into the DPO objective.\n","\n","**Intuition**: It quantifies how confident the model is in a sequence. Larger log-probability ⇒ the model finds the sequence more plausible.\n","\n","### 2. `pad_or_truncate(seq, max_length)`\n","\n","Normalizes sequence length by trimming long sequences and padding shorter ones with zeros so tensors align in a batch.\n","\n","### 3. `get_batches(lines, batch_size)`\n","\n","Builds mini-batches of preference pairs for training. Each batch bundles matched **(positive, negative)** examples—where the positive is the correct reasoning/answer and the negative is the incorrect one.\n"]},{"cell_type":"code","execution_count":null,"id":"d03655c3","metadata":{"id":"d03655c3"},"outputs":[],"source":["def compute_logprob(input_ids):\n","    inputs = input_ids[:, :-1]\n","    targets = input_ids[:, 1:]\n","    logits, _ = gpt(inputs, full_seq=True)\n","    B, T, V = logits.size()\n","    logits_flat = logits.reshape(-1, V)\n","    targets_flat = targets.reshape(-1)\n","    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=PAD_ID, reduction='none')   #added ignore_index=PAD_ID\n","    loss = loss.reshape(B, T)\n","    attention_mask = (targets != 0).float()\n","    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n","    return -loss\n","\n","def pad_or_truncate(seq, max_length):\n","    return seq[-max_length:] if len(seq) > max_length else seq + [PAD_ID] * (max_length - len(seq))\n","\n","def get_batches(lines, batch_size):\n","    random.shuffle(lines)\n","    #for l in lines:\n","    #    print(l[1])\n","    for i in range(0, len(lines), batch_size):\n","        batch = lines[i:i+batch_size]\n","        if len(batch) < batch_size:\n","            continue\n","        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n","        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n","        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n","        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n","        yield neg_tensor, pos_tensor"]},{"cell_type":"markdown","id":"fc9d9eba","metadata":{"id":"fc9d9eba"},"source":["## Step 4: Load the pretrained NanoGPT model\n","Bring in a NanoGPT checkpoint that’s already been pretrained to use as the initialization for DPO fine-tuning."]},{"cell_type":"code","execution_count":null,"id":"ceae772a","metadata":{"id":"ceae772a"},"outputs":[],"source":["ckpt = torch.load(\"sft/gpt.pt\", map_location=device)\n","gptconf = GPTConfig(**ckpt['model_args'])\n","gpt = GPT(gptconf)\n","state_dict = ckpt['model']\n","unwanted_prefix = '_orig_mod.'\n","for k in list(state_dict.keys()):\n","    if k.startswith(unwanted_prefix):\n","        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","gpt.load_state_dict(state_dict)\n","gpt.to(device).train()"]},{"cell_type":"markdown","id":"cQz1CNxcVjQ4","metadata":{"id":"cQz1CNxcVjQ4"},"source":["## Step 5: Load Data\n","\n","Import the DPO dataset of **preference pairs**. Each record includes:\n","\n","* **`positive`**: a correct solution with clear, valid reasoning\n","* **`negative`**: an incorrect answer or weak/flawed reasoning\n","\n","The corpus contains **100,000 pairs**, giving ample supervision for the model to learn preference alignment.\n","\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"d7C1ludbDYZv"},"id":"d7C1ludbDYZv"},{"cell_type":"code","execution_count":null,"id":"b4sAFYAeur9b","metadata":{"id":"b4sAFYAeur9b"},"outputs":[],"source":["import torch, pickle, os\n","from model import GPT, GPTConfig\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","if 'gpt' not in globals() or 'encode' not in globals() or 'decode' not in globals():\n","    with open(\"sft/meta.pkl\", \"rb\") as f:\n","        meta = pickle.load(f)\n","    stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n","\n","    def encode(s): return [stoi[c] for c in s]\n","    def decode(l): return ''.join([itos[i] for i in l])\n","\n","    ckpt = torch.load(\"sft/gpt.pt\", map_location=device)\n","    gptconf = GPTConfig(**ckpt['model_args'])\n","    gpt = GPT(gptconf)\n","    sd = ckpt['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k in list(sd.keys()):\n","        if k.startswith(unwanted_prefix):\n","            sd[k[len(unwanted_prefix):]] = sd.pop(k)\n","    gpt.load_state_dict(sd)\n","    gpt.to(device).eval()\n","\n","max_length    = globals().get('max_length', 64)\n","temperature   = globals().get('temperature', 0.8)\n","top_k         = globals().get('top_k', 200)\n","max_new_tokens= min(globals().get('max_new_tokens', 60), 120)\n"]},{"cell_type":"code","execution_count":null,"id":"4sSyT_IhvG6n","metadata":{"id":"4sSyT_IhvG6n"},"outputs":[],"source":["import torch\n","\n","@torch.no_grad()\n","def sample_from_gpt(prompt: str,\n","                    max_new: int = max_new_tokens,\n","                    temp: float = temperature,\n","                    topk: int = top_k) -> str:\n","    gpt.eval()\n","    ids = encode(prompt)\n","    x = torch.tensor([ids[-max_length:]], dtype=torch.long, device=device)\n","    generated = []\n","\n","    for _ in range(max_new):\n","        logits, _ = gpt(x)\n","        logits = logits[:, -1, :]\n","        if temp and temp > 0:\n","            logits = logits / temp\n","        if topk and 0 < topk < logits.size(-1):\n","            v, _ = torch.topk(logits, topk)\n","            thresh = v[:, -1].unsqueeze(-1)\n","            logits[logits < thresh] = -float('inf')\n","        probs = torch.softmax(logits, dim=-1)\n","        next_id = torch.multinomial(probs, num_samples=1)\n","        token_id = int(next_id.item())\n","        generated.append(token_id)\n","\n","        ch = itos[token_id]\n","        if ch in ['\\n', '\\r']:\n","            break\n","\n","        x = torch.cat([x, next_id], dim=1)\n","        if x.size(1) > max_length:\n","            x = x[:, -max_length:]\n","\n","    return ''.join(itos[i] for i in generated).strip()\n"]},{"cell_type":"code","execution_count":null,"id":"1_73NpPQvHxl","metadata":{"id":"1_73NpPQvHxl"},"outputs":[],"source":["import random\n","\n","def _int_div_triplet():\n","    b = random.randint(1, 12)\n","    x = random.randint(1, 12)\n","    a = b * x\n","    return a, b, x\n","\n","def generate_problem():\n","    mode = random.choice([\n","        \"arith_add\",\"arith_sub\",\"arith_mul\",\"arith_div\",\n","        \"alg_x_mul\",\"alg_mul_x\",\"alg_x_add\",\"alg_add_x\",\n","        \"alg_x_sub\",\"alg_sub_x\",\"alg_div_x\",\"alg_x_div\"\n","    ])\n","\n","    if mode == \"arith_add\":\n","        a,b = random.randint(-50,99), random.randint(-50,99)\n","        ans = a + b; prompt = f\"{a}+{b}, x=?\"\n","        expl = f\"{a}+{b} equals {ans}.\"\n","    elif mode == \"arith_sub\":\n","        a,b = random.randint(-50,99), random.randint(-50,99)\n","        ans = a - b; prompt = f\"{a}-{b}, x=?\"\n","        expl = f\"{a}-{b} equals {ans}.\"\n","    elif mode == \"arith_mul\":\n","        a,b = random.randint(-12,12), random.randint(-12,12)\n","        ans = a * b; prompt = f\"{a}*{b}, x=?\"\n","        expl = f\"{a}*{b} equals {ans}.\"\n","    elif mode == \"arith_div\":\n","        a,b,x = _int_div_triplet()\n","        ans = a // b; prompt = f\"{a}/{b}, x=?\"\n","        expl = f\"{a}/{b} equals {ans}.\"\n","    elif mode == \"alg_x_mul\":            # x*b=a\n","        a,b,x = _int_div_triplet()\n","        ans = x; prompt = f\"x*{b}={a}, x=?\"\n","        expl = f\"{a}/{b} equals {ans}.\"\n","    elif mode == \"alg_mul_x\":            # b*x=a\n","        a,b,x = _int_div_triplet()\n","        ans = x; prompt = f\"{b}*x={a}, x=?\"\n","        expl = f\"{a}/{b} equals {ans}.\"\n","    elif mode == \"alg_x_add\":            # x+b=a\n","        b = random.randint(-50,99); x = random.randint(-50,99); a = x + b\n","        ans = x; prompt = f\"x+{b}={a}, x=?\"\n","        expl = f\"{a}-{b} equals {ans}.\"\n","    elif mode == \"alg_add_x\":            # b+x=a\n","        b = random.randint(-50,99); x = random.randint(-50,99); a = b + x\n","        ans = x; prompt = f\"{b}+x={a}, x=?\"\n","        expl = f\"{a}-{b} equals {ans}.\"\n","    elif mode == \"alg_x_sub\":            # x-b=a => x=a+b\n","        b = random.randint(-50,99); x = random.randint(-50,99); a = x - b\n","        ans = x; prompt = f\"x-{b}={a}, x=?\"\n","        expl = f\"{a}+{b} equals {ans}.\"\n","    elif mode == \"alg_sub_x\":            # b-x=a => x=b-a\n","        a = random.randint(-50,99); b = random.randint(-50,99); x = b - a\n","        ans = x; prompt = f\"{b}-x={a}, x=?\"\n","        expl = f\"{b}-{a} equals {ans}.\"\n","    elif mode == \"alg_div_x\":            # a/x=b => x=a/b\n","        a,b,x = _int_div_triplet()\n","        ans = x; prompt = f\"{a}/x={b}, x=?\"\n","        expl = f\"{a}/{b} equals {ans}.\"\n","    else:                                # x/b=a => x=a*b\n","        b = random.randint(1,12); a = random.randint(-12,12); x = a*b\n","        ans = x; prompt = f\"x/{b}={a}, x=?\"\n","        expl = f\"{a}*{b} equals {ans}.\"\n","\n","    return prompt, ans, expl\n","\n","def build_positive(prompt: str, ans: int, expl: str) -> str:\n","    return f\"{prompt} The answer is {ans} because {expl}\"\n"]},{"cell_type":"code","execution_count":null,"id":"4UVfjk5fvLOK","metadata":{"id":"4UVfjk5fvLOK"},"outputs":[],"source":["def tidy_model_reply(text: str) -> str:\n","    \"\"\"\n","    Keep first line, clamp length, fallback to a safe string if junk/empty.\n","    \"\"\"\n","    t = text.replace('\\r','\\n').split('\\n')[0].strip()\n","    if not t:\n","        return \"Sorry, I do not know!\"\n","    if len(t) > 160:\n","        t = t[:160].rstrip()\n","    bad = [\"<|\", \"|>\", \"[INST]\", \"[/INST]\"]\n","    if any(b in t for b in bad):\n","        return \"Sorry, I do not know!\"\n","    return t\n"]},{"cell_type":"code","execution_count":null,"id":"l0zz5y4-vPjA","metadata":{"id":"l0zz5y4-vPjA"},"outputs":[],"source":["import os, json\n","from tqdm import tqdm\n","\n","N_SAMPLES = 100_000\n","OUT_PATH  = \"dpo/pos_neg_pairs.json\"\n","os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n","\n","pairs = []\n","for _ in tqdm(range(N_SAMPLES)):\n","    prompt, ans, expl = generate_problem()\n","\n","    # Negative via pretrained NanoGPT\n","    neg_cont = sample_from_gpt(prompt, max_new=max_new_tokens)\n","    neg_text = tidy_model_reply(neg_cont)\n","    negative = f\"{prompt} {neg_text}\"\n","\n","    # Positive via solver (human-preference)\n","    positive = build_positive(prompt, ans, expl)\n","\n","    pairs.append({\"negative\": negative, \"positive\": positive})\n","\n","with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(pairs, f, ensure_ascii=True, indent=2)\n","\n","print(f\"Generated {len(pairs)} pairs → {OUT_PATH}\")\n"]},{"cell_type":"code","execution_count":null,"id":"7edf3d44","metadata":{"id":"7edf3d44"},"outputs":[],"source":["import json\n","\n","# Path to the data file\n","# We use \"dpo/pos_neg_pairs.json\" because working directory is \"NanoGPT-Math\"\n","data_path = \"dpo/pos_neg_pairs.json\"\n","\n","# Open and load the JSON file\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = json.load(f)\n","\n","print(f\"Successfully loaded {len(lines)} data pairs from {data_path}\")\n","print(\"First pair:\", lines[0])"]},{"cell_type":"markdown","id":"c2e5f81f","metadata":{"id":"c2e5f81f"},"source":["## Step 6: Configure the Optimizer and LR Schedule\n","\n","### AdamW Optimizer\n","\n","Use **AdamW**, the go-to optimizer for Transformers. It adds **decoupled weight decay**, which acts as regularization to curb overfitting while keeping Adam’s adaptive updates.\n","\n","### Cosine Annealing Schedule\n","\n","Apply a **cosine decay** to the learning rate so it:\n","\n","* Begins with relatively larger steps for quick early gains\n","* Tapers to smaller steps for fine-grained refinement\n","* Improves convergence by reducing the chance of overshooting the optimum\n"]},{"cell_type":"code","execution_count":null,"id":"ANnzVfTCPNai","metadata":{"id":"ANnzVfTCPNai"},"outputs":[],"source":["# --- 1. Install the 'transformers' library for the scheduler ---\n","!pip install transformers\n","\n","# --- 2. Import everything ---\n","from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","# --- 3. Define hyperparameters (make sure 'batch_size' is set!) ---\n","epochs = 20\n","batch_size = 16  # <-- MAKE SURE THIS IS DEFINED\n","base_lr = 1e-4     # <-- THIS WAS THE MISSING VARIABLE\n","\n","# --- 4. Calculate steps (this depends on 'lines' from Step 5) ---\n","num_batches = len(lines) // batch_size\n","total_steps = num_batches * epochs\n","\n","# --- 5. Create optimizer and scheduler ---\n","optimizer = AdamW(gpt.parameters(), lr=base_lr) # <-- Use optimizer, not optim\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.15 * total_steps),\n","    num_training_steps=total_steps\n",")\n","\n","# --- 6. Print to confirm (changed 'rint' to 'print') ---\n","print(f\"Optimizer and scheduler created.\")\n","print(f\"Total training steps: {total_steps}\")"]},{"cell_type":"code","execution_count":null,"id":"hO-b1CRwPZ3-","metadata":{"id":"hO-b1CRwPZ3-"},"outputs":[],"source":["import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","# Assumes 'gpt', 'learning_rate', and 'epochs' are already defined\n","optimizer = optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=0.10, betas=(0.9, 0.95), eps=1e-8)\n","\n","# T_max=epochs means the learning rate will gradually decrease over the total number of epochs\n","scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n","\n","print(\"AdamW optimizer and CosineAnnealingLR scheduler created successfully.\")\n"]},{"cell_type":"markdown","id":"52b66199","metadata":{"id":"52b66199"},"source":["### Step 7: Begin training (**students are required to complete this part!**)"]},{"cell_type":"code","execution_count":null,"id":"5XQT-yT8Ug5w","metadata":{"id":"5XQT-yT8Ug5w"},"outputs":[],"source":["import tensorflow as tf\n","tf.test.gpu_device_name()\n"]},{"cell_type":"code","source":["# choose a safe fallback char that is definitely in your vocab\n","SAFE_CHAR = ' ' if ' ' in stoi else '\\n'\n","\n","def encode(s):\n","    fallback_id = stoi[SAFE_CHAR]\n","    return [stoi.get(c, fallback_id) for c in s]\n"],"metadata":{"id":"UcZEeRqWB0M3"},"id":"UcZEeRqWB0M3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1d4ebeb4","metadata":{"id":"1d4ebeb4"},"outputs":[],"source":["import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_norm_\n","from tqdm import tqdm\n","\n","# if beta wasn't set earlier, provide a sensible default\n","beta = 0.1 if 'beta' not in globals() else beta\n","\n","total_steps = len(lines) // batch_size\n","for epoch in range(epochs):\n","    pbar = tqdm(get_batches(lines, batch_size), total=total_steps, desc=f\"Epoch {epoch+1}/{epochs}\")\n","    for step, (neg_tensor, pos_tensor) in enumerate(pbar, start=1):\n","        ###########################################################\n","        # Completed training code\n","        ###########################################################\n","        # 1) move to device\n","        neg_tensor = neg_tensor.to(device)\n","        pos_tensor = pos_tensor.to(device)\n","\n","        # 2) zero grad\n","        optimizer.zero_grad()\n","\n","        # 3) forward: per-sample log-probabilities\n","        #    (compute_logprob should be defined in your Step 3)\n","        neg_logprob = compute_logprob(neg_tensor)   # shape [B]\n","        pos_logprob = compute_logprob(pos_tensor)   # shape [B]\n","\n","        # 4) DPO-style loss\n","        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n","\n","        # 5) backward\n","        loss.backward()\n","        clip_grad_norm_(gpt.parameters(), 1.0)\n","\n","        # 6) optimizer step\n","        optimizer.step()\n","\n","        # progress\n","        lr_now = optimizer.param_groups[0]['lr']\n","        pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{lr_now:.2e}\")\n","\n","        # keep the progress bar aligned with total_steps\n","        if step >= total_steps:\n","            break\n","    ###########################################################\n","\n","    # step the scheduler once per epoch (matches CosineAnnealingLR in Step 6)\n","    if 'scheduler' in globals():\n","        scheduler.step()\n","\n","    # save checkpoint (guard model_args in case ckpt isn't in scope)\n","    ckpt_path = f\"./dpo.pt\"\n","    model_args = ckpt['model_args'] if ('ckpt' in globals() and isinstance(ckpt, dict) and 'model_args' in ckpt) else None\n","    torch.save({\n","        \"model_state_dict\": gpt.state_dict(),\n","        \"model_args\": model_args,\n","    }, ckpt_path)\n","    print(f\"Saved checkpoint to {ckpt_path} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","print(\"Training finished.\")"]},{"cell_type":"markdown","id":"48b7f2ab","metadata":{"id":"48b7f2ab"},"source":["### Step 8: Model Evaluation and Tewsting\n","\n","Test the model on a small set of example problems covering different opeartion types. This quick test helps verify the model is working correctly."]},{"cell_type":"code","execution_count":null,"id":"09027262","metadata":{"id":"09027262"},"outputs":[],"source":["import os, torch\n","import torch.nn.functional as F\n","\n","# 1) Load the fine-tuned model\n","ckpt_path = \"./dpo.pt\"\n","assert os.path.exists(ckpt_path), f\"Checkpoint not found at {ckpt_path}. Run Step 7 first.\"\n","\n","checkpoint = torch.load(ckpt_path, map_location=device)\n","gptconf = GPTConfig(**checkpoint['model_args'])\n","gpt = GPT(gptconf).to(device)\n","\n","# state dict (supports either key name)\n","state_dict = checkpoint.get('model', checkpoint.get('model_state_dict'))\n","unwanted_prefix = '_orig_mod.'\n","for k in list(state_dict.keys()):\n","    if k.startswith(unwanted_prefix):\n","        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","\n","gpt.load_state_dict(state_dict, strict=True)\n","\n","# 2) Test\n","gpt.eval()\n","test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n","\n","with torch.no_grad():\n","    for prompt in test_set:\n","        prompt_ids = encode(prompt)\n","        x_ids = prompt_ids[-max_length:]\n","        x = torch.tensor([x_ids], dtype=torch.long, device=device)\n","\n","        # generate continuation\n","        y, _ = gpt.generate(\n","            x,\n","            max_new_tokens=max_new_tokens,\n","            temperature=temperature,\n","            top_k=top_k\n","        )\n","\n","        # decode: print only the newly generated part (cleaner), plus an optional full line\n","        out_ids = y[0].tolist()\n","        new_tokens_only = out_ids[len(x_ids):]\n","        out = decode(new_tokens_only)\n","\n","        print(f\"{prompt} → {out.strip()}\")\n","\n"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python [conda env:gpu_env]","name":"conda-env-gpu_env-py"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}